# Ultralytics YOLO ğŸš€, AGPL-3.0 license
"""Convolution modules."""

import math

import numpy as np
import torch
import torch.nn as nn

__all__ = (
    "Conv",
    "Conv2",
    "LightConv",
    "DWConv",
    "DWConvTranspose2d",
    "ConvTranspose",
    "Focus",
    "GhostConv",
    "ChannelAttention",
    "SpatialAttention",
    "CBAM",
    "Concat",
    "RepConv",
)


def autopad(k, p=None, d=1):  # kernel, padding, dilation
    """Pad to 'same' shape outputs.è¿”å›padçš„å¤§å°ï¼Œä½¿å¾—paddingåè¾“å‡ºå¼ é‡çš„å¤§å°ä¸å˜
        k: (kernel) int or åºåˆ—ï¼Œå·ç§¯æ ¸çš„å¤§å°
        p: (padding) None, å¡«å……çš„å¤§å°
        d: (dilation rate), é»˜è®¤ä¸º1, æ‰©å¼ ç‡çš„å¤§å°ã€‚æ™®é€šå·ç§¯çš„æ‰©å¼ ç‡ä¸º1, ç©ºæ´å·ç§¯çš„æ‰©å¼ ç‡å¤§äº1    
    """
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
        # åŠ å…¥ç©ºæ´ä¹‹åçš„å®é™…å·ç§¯æ ¸å°ºå¯¸ä¸åŸå§‹å·ç§¯æ ¸å°ºå¯¸ä¹‹é—´çš„å…³ç³»: k=d(k-1)+1
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
        # //æ˜¯å‘ä¸‹æ•´é™¤è¿ç®—ï¼Œç±»ä¼¼äºmath.floor()ã€‚ç¤ºä¾‹å³29//10=2
    return p


class Conv(nn.Module):
    """Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).
        c1: è¾“å…¥é€šé“æ•°; c2: è¾“å‡ºé€šé“æ•°(å·ç§¯æ ¸çš„æ•°é‡å°±æ˜¯c2); k: å·ç§¯æ ¸çš„å¤§å°;
        s: æ­¥é•¿ï¼Œé»˜è®¤ä¸º1; p: å¡«å……ï¼Œé»˜è®¤ä¸ºNone; g: ç»„ï¼Œé»˜è®¤ä¸º1;
        d: æ‰©å¼ ç‡ï¼Œé»˜è®¤ä¸º1; act: æ˜¯å¦é‡‡ç”¨æ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸ºTrueï¼Œä¸”é‡‡ç”¨SiLUä¸ºæ¿€æ´»å‡½æ•°;
        SiLU(x)=x(1/(1+exp(-x)))
        å¯¹äºgroupå‚æ•°ï¼Œå‡å¦‚group=2ï¼Œç­‰æ•ˆäºå¹¶æ’ä¸¤ä¸ªå·ç§¯å±‚ï¼Œæ¯ä¸ªå±‚è¾“å…¥1/2c1å¹¶è¾“å‡º1/2c2ï¼Œå¹¶ä¸”éšåå°†äºŒè€…è¿èµ·æ¥
        ç©ºæ´å·ç§¯å±‚ä¸ä¸€èˆ¬å·ç§¯é—´çš„å·®åˆ«åœ¨äºè†¨èƒ€ç‡ï¼Œè†¨èƒ€ç‡æ§åˆ¶çš„æ˜¯å·ç§¯æ—¶çš„ padding ä»¥åŠ dilationã€‚
            é€šè¿‡ä¸åŒçš„å¡«å……ä»¥åŠä¸è†¨èƒ€ï¼Œå¯ä»¥è·å–ä¸åŒå°ºåº¦çš„æ„Ÿå—é‡ï¼Œæå–å¤šå°ºåº¦çš„ä¿¡æ¯ã€‚
    """

    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        """Initialize Conv layer with given arguments including activation."""
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)    # å‡å€¼ä¸º0, æ–¹å·®ä¸º1
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
        # å¦‚æœact=True, åˆ™é‡‡ç”¨é»˜è®¤çš„æ¿€æ´»å‡½æ•°SiLU; å¦‚æœactçš„ç±»å‹æ˜¯nn.Module, åˆ™é‡‡ç”¨ä¼ å…¥çš„act;
        # å¦åˆ™ä¸é‡‡å–ä»»ä½•åŠ¨ä½œ(nn.Identityå‡½æ•°ç›¸å½“äºf(x)=x, åªç”¨åšå ä½, è¿”å›åŸå§‹çš„è¾“å…¥)

    def forward(self, x):   # æ ‡å‡†çš„å‰å‘ä¼ æ’­ç®—æ³•ï¼Œé€šå¸¸åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶éƒ½ä½¿ç”¨
        """Apply convolution, batch normalization and activation to input tensor."""
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):  # ç”¨Moduleç±»çš„fuseå‡½æ•°èåˆConv+BNåŠ é€Ÿæ¨ç†, ä¸€èˆ¬ç”¨äºæµ‹è¯•/éªŒè¯é˜¶æ®µ
        # è¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–çš„å‰å‘ä¼ æ’­æ–¹æ³•, å°†BNæ‰¹å½’ä¸€åŒ–å±‚å’Œconvå·ç§¯å±‚èåˆ, é€šå¸¸ç”¨äºæ¨ç†é˜¶æ®µ
        """Perform transposed convolution of 2D data."""
        return self.act(self.conv(x))


class Conv2(Conv):
    """Simplified RepConv module with Conv fusing.
        å®ç°æ™®é€šå·ç§¯å’Œ1*1å·ç§¯çš„å åŠ 
    """

    def __init__(self, c1, c2, k=3, s=1, p=None, g=1, d=1, act=True):
        """Initialize Conv layer with given arguments including activation."""
        super().__init__(c1, c2, k, s, p, g=g, d=d, act=act)
        self.cv2 = nn.Conv2d(c1, c2, 1, s, autopad(1, p, d), groups=g, dilation=d, bias=False)  # add 1x1 conv
        # cv2è®¾ç½®çš„æ˜¯1*1çš„å·ç§¯ï¼Œå¯ç”¨äºæ•´åˆç‰¹å¾å›¾ä¸åŒé€šé“ä¹‹é—´çš„ä¿¡æ¯ã€‚åªåœ¨é€šé“ç»´åº¦ä¸Šè¿›è¡Œè®¡ç®—ï¼Œä»è€Œæ··åˆå„é€šé“çš„ä¿¡æ¯ã€‚èƒ½ç”¨æ¥å‡ç»´æˆ–é™ç»´

    def forward(self, x):
        """Apply convolution, batch normalization and activation to input tensor."""
        return self.act(self.bn(self.conv(x) + self.cv2(x)))
        # ä½¿ç”¨ä¸¤ä¸ªå·ç§¯æ“ä½œ(1ä¸ªæ™®é€šå·ç§¯å’Œ1ä¸ª1*1å·ç§¯),å¹¶å°†å…¶ç»“æœç›¸åŠ 
        # å¢åŠ æ¨¡å‹å¤æ‚æ€§å’Œè¡¨è¾¾èƒ½åŠ›ï¼Œç”¨äºè®­ç»ƒé˜¶æ®µï¼Œå……åˆ†åˆ©ç”¨æ¨¡å‹çš„å¤æ‚æ€§

    def forward_fuse(self, x):
        """Apply fused convolution, batch normalization and activation to input tensor."""
        return self.act(self.bn(self.conv(x)))
        # ä»…ä½¿ç”¨ä¸€ä¸ªå·ç§¯æ“ä½œï¼Œé€‚ç”¨äºéœ€è¦ç®€åŒ–è®¡ç®—å’ŒåŠ é€Ÿæ¨ç†çš„åœºæ™¯ï¼Œç”¨äºæ¨ç†é˜¶æ®µï¼Œå‡å°‘è®¡ç®—é‡å’Œæé«˜æ¨ç†é€Ÿåº¦

    def fuse_convs(self):
        """Fuse parallel convolutions.
            ç›®çš„æ˜¯å°†ä¸¤ä¸ªå¹¶è¡Œçš„å·ç§¯æ“ä½œèåˆæˆä¸€ä¸ªï¼Œä»¥ç®€åŒ–æ¨¡å‹ç»“æ„å¹¶æé«˜æ¨ç†é€Ÿåº¦
            å°†self.cv2(1*1å·ç§¯)çš„å·ç§¯æ ¸æƒé‡æ•´åˆåˆ°self.convçš„æƒé‡ä¸­ï¼Œå¹¶åˆ é™¤self.cv2å±‚ï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µåªæ‰§è¡Œä¸€æ¬¡å·ç§¯æ“ä½œ
        """
        w = torch.zeros_like(self.conv.weight.data) # wç”¨äºå­˜å‚¨self.cv2çš„æƒé‡æ•°æ® 
            # å·ç§¯å±‚çš„æƒé‡é€šå¸¸ä¸ºæ€ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º(OutChannels, InChannels, KernelHeight, KernelWidth)
        i = [x // 2 for x in w.shape[2:]]   # å–å‡º(KernelHeight, KernelWidth)å·ç§¯æ ¸çš„é«˜åº¦å’Œå®½åº¦ï¼Œå¹¶è®¡ç®—ä»–ä»¬çš„ä¸€åŠ(å·ç§¯æ ¸çš„ä¸­å¿ƒ)
        w[:, :, i[0] : i[0] + 1, i[1] : i[1] + 1] = self.cv2.weight.data.clone()    # å°†self.cv2çš„æƒé‡æ•°æ®æ”¾åœ¨wçš„ä¸­å¿ƒä½ç½®
            # cloneæ–¹æ³•ä¸ºäº†ç¡®ä¿å¤åˆ¶çš„æ˜¯æ•°æ®çš„å‰¯æœ¬ 
        self.conv.weight.data += w  # å°†åµŒå…¥äº†self.cv2çš„æƒé‡wåŠ åˆ°äº†self.convçš„æƒé‡ä¸Šï¼Œä»è€Œå°†self.cv2çš„æƒé‡æ•´åˆè¿›self.conv
        self.__delattr__("cv2") # åˆ é™¤äº†self.cv2å±æ€§ï¼Œèåˆåä¸å†éœ€è¦self.cv2å±‚
        self.forward = self.forward_fuse    # æ›´æ–°forwardæ–¹æ³•ï¼Œä½¿å…¶æŒ‡å‘forward_fuse.ä½¿æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µåªæ‰§è¡Œä¸€æ¬¡å·ç§¯æ“ä½œï¼Œä¸æ¶‰åŠ1*1å·ç§¯


class LightConv(nn.Module):
    """
    Light convolution with args(ch_in, ch_out, kernel).

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py
    å®ç°æ·±åº¦å¯åˆ†ç¦»å·ç§¯(Depthwise Separable Convolution)
    å³åœ¨æ·±åº¦å·ç§¯ä¹‹åï¼Œé€šè¿‡Pointwise Convolutionæ“ä½œç”Ÿæˆæ–°çš„Feature Map
    ç”±æ·±åº¦å·ç§¯å’Œé€ç‚¹å·ç§¯(1*1å·ç§¯)ç»„æˆï¼Œæ·±åº¦å·ç§¯ç”¨äºæå–ç©ºé—´ç‰¹å¾ï¼Œé€ç‚¹å·ç§¯ç”¨äºæå–é€šé“ç‰¹å¾
    """

    def __init__(self, c1, c2, k=1, act=nn.ReLU()):
        """Initialize Conv layer with given arguments including activation."""
        super().__init__()
        self.conv1 = Conv(c1, c2, 1, act=False) # åˆ›å»º1*1å·ç§¯ï¼Œå³Pointwise Convolution(å°†è¾“å…¥åœ¨æ·±åº¦æ–¹å‘è¿›è¡ŒåŠ æƒç»„åˆ)
        self.conv2 = DWConv(c2, c2, k, act=act) # Depthwise Convolution

    def forward(self, x):
        """Apply 2 convolutions to input tensor."""
        return self.conv2(self.conv1(x))    # ä¾æ¬¡åº”ç”¨1*1å·ç§¯å’Œæ·±åº¦å·ç§¯æå–ç‰¹å¾
            # å‡å°‘è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒä¸€å®šçš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œé€‚ç”¨äºéœ€è¦é™ä½æ¨¡å‹å¤æ‚åº¦åŒæ—¶ä¿æŒç²¾åº¦çš„åœºæ™¯


class DWConv(Conv):
    """Depth-wise convolution.
        æ·±åº¦å·ç§¯ï¼šä¸€ä¸ªå·ç§¯æ ¸è´Ÿè´£ä¸€ä¸ªé€šé“ï¼Œä¸€ä¸ªé€šé“åªè¢«ä¸€ä¸ªå·ç§¯æ ¸å·ç§¯
                 å·ç§¯æ ¸çš„æ•°é‡ä¸ä¸Šä¸€å±‚çš„é€šé“æ•°ç›¸åŒï¼ˆé€šé“æ ¸å·ç§¯æ ¸ä¸€ä¸€å¯¹åº”ï¼‰
                 æ— æ³•æ‰©å±•Feature Mapï¼Œå¯¹è¾“å…¥å±‚çš„æ¯ä¸ªé€šé“ç‹¬ç«‹è¿›è¡Œå·ç§¯è¿ç®—ï¼Œæ²¡æœ‰æœ‰æ•ˆåˆ©ç”¨ä¸åŒé€šé“åœ¨ç›¸åŒç©ºé—´ä½ç½®çš„featureä¿¡æ¯
                 é€šå¸¸éœ€è¦Pointwise Convolutionå°†è¿™äº›Feature Mapç”Ÿæˆæ–°çš„Feature Map
    """

    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):  # ch_in, ch_out, kernel, stride, dilation, activation
        """Initialize Depth-wise convolution with given parameters."""
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)
        # math.gcdè®¡ç®—c2(è¾“å‡ºé€šé“)ä¸c1(è¾“å…¥é€šé“)çš„æœ€å¤§å…¬çº¦æ•°--æ·±åº¦å·ç§¯çš„å…³é”®
        # è®¾ç½®ä¸ºæœ€å¤§å…¬çº¦æ•°ï¼Œç¡®ä¿äº†ç»„æ•°èƒ½å¤ŸåŒæ—¶æ•´é™¤è¾“å…¥é€šé“å’Œè¾“å‡ºé€šé“ï¼Œä»è€Œå…è®¸æ¯ä¸ªæ»¤æ³¢å™¨å¤„ç†å•ä¸ªè¾“å…¥é€šé“ï¼ŒåŒæ—¶ä»ç„¶ç”Ÿæˆæ‰€éœ€çš„è¾“å‡ºé€šé“


class DWConvTranspose2d(nn.ConvTranspose2d):
    """Depth-wise transpose convolution.
        è¯¥ç±»åªæ˜¯æŠŠ"æ·±åº¦"æ“ä½œDepthwiseç”¨åœ¨è½¬ç½®å·ç§¯ä¸Š
    """

    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out
        """Initialize DWConvTranspose2d class with given parameters.
            p1è¾“å…¥å¡«å……ï¼Œåœ¨è½¬ç½®å·ç§¯æ“ä½œä¹‹å‰åº”ç”¨äºè¾“å…¥ç‰¹å¾å›¾ï¼Œå¯ä»¥æ§åˆ¶è¾“å…¥ç‰¹å¾å›¾è¾¹ç•Œå¦‚ä½•å¤„ç†(é›¶å¡«å……æˆ–é•œåƒå¡«å……)
                ä»¥åŠå¡«å……çš„å®½åº¦ï¼Œä¼šå½±å“è¾“å‡ºç‰¹å¾å›¾çš„å¤§å°ã€‚
            p2è¾“å‡ºå¡«å……ï¼Œåœ¨è½¬ç½®å·ç§¯æ“ä½œä¹‹ååº”ç”¨äºè¾“å‡ºç‰¹å¾å›¾ï¼Œä¸ä¼šå½±å“è¾“å‡ºç‰¹å¾å›¾çš„é€šé“æ•°ï¼Œè€Œæ˜¯ä¼šæ”¹å˜å…¶é«˜å’Œå®½ç»´åº¦çš„å¤§å°
        """
        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))
        # å®šä¹‰äº†groupsä¸ºc1å’Œc2çš„æœ€å°å…¬çº¦æ•°ï¼Œå®ç°æ·±åº¦è½¬ç½®å·ç§¯çš„å…³é”®
        # ä½¿å¾—æ¯ä¸€ä¸ªè½¬ç½®å·ç§¯è¿‡æ»¤å™¨éƒ½åªåº”ç”¨äºä¸€å±‚è¾“å…¥å±‚ï¼Œå¯å‡å°è½¬ç½®å·ç§¯æ‰€éœ€çš„å‚æ•°


class ConvTranspose(nn.Module):
    """Convolution transpose 2d layer.
        è½¬ç½®å·ç§¯ï¼Œé€‚ç”¨äºéœ€è¦è¿›è¡Œä¸Šé‡‡æ ·æ“ä½œæˆ–é‡å»ºä¸¢å¤±ä¿¡æ¯çš„åœºæ™¯ã€‚
        è½¬ç½®å·ç§¯åˆç§°åå·ç§¯(Deconvolution)ï¼Œå…¶ä¸Šé‡‡æ ·æ–¹å¼å¹¶éé¢„è®¾çš„æ’å€¼æ–¹æ³•ï¼Œè€Œæ˜¯å…·æœ‰å¯å­¦ä¹ çš„å‚æ•°ï¼Œå¯é€šè¿‡ç½‘ç»œå­¦ä¹ è·å–æœ€ä¼˜çš„ä¸Šé‡‡æ ·æ–¹å¼
        å¸¸è§„å·ç§¯çš„æ“ä½œæ˜¯ä¸å¯é€†çš„ï¼Œæ‰€ä»¥è½¬ç½®å·ç§¯å¹¶ä¸æ˜¯é€šè¿‡è¾“å‡ºçŸ©é˜µå’Œå·ç§¯æ ¸è®¡ç®—åŸå§‹è¾“å…¥çŸ©é˜µï¼Œè€Œæ˜¯è®¡ç®—å¾—åˆ°ä¿æŒäº†ç›¸å¯¹ä½ç½®å…³ç³»çš„çŸ©é˜µ
        çŸ©é˜µä¸­çš„å®é™…æƒå€¼ä¸ä¸€å®šæ¥è‡ªåŸå§‹å·ç§¯çŸ©é˜µï¼Œä½†æƒé‡çš„æ’å¸ƒç”±å·ç§¯çŸ©é˜µçš„è½¬ç½®çš„æ¥ã€‚è½¬ç½®å·ç§¯ä¸æ™®é€šå·ç§¯å½¢æˆç›¸åŒçš„è¿é€šæ€§ä½†æ–¹å‘ç›¸å
        è½¬ç½®å·ç§¯ä¸æ˜¯å·ç§¯ï¼Œä½†å¯ä»¥ç”¨å·ç§¯æ¥æ¨¡æ‹Ÿè½¬ç½®å·ç§¯ã€‚é€šè¿‡åœ¨è¾“å…¥çŸ©é˜µçš„å€¼é—´æ’å…¥é›¶å€¼(ä»¥åŠå‘¨å›´å¡«é›¶)ä¸Šé‡‡æ ·è¾“å…¥çŸ©é˜µï¼Œç„¶åè¿›è¡Œå¸¸è§„å·ç§¯ï¼Œ
            å°±ä¼šäº§ç”Ÿä¸è½¬ç½®å·ç§¯ç›¸åŒçš„æ•ˆæœ
        æ³¨æ„ï¼šè½¬ç½®å·ç§¯ä¼šå¯¼è‡´ç”Ÿæˆå›¾åƒä¸­å‡ºç°çš„ç½‘æ ¼/æ£‹ç›˜æ•ˆåº”(checkerboard artifacts)
                æ£‹ç›˜æ•ˆåº”ç”±äºåå·ç§¯çš„"ä¸å‡åŒ€é‡å (Uneven overlap)"çš„ç»“æœï¼Œä½¿å›¾åƒä¸­æŸä¸ªéƒ¨ä½çš„é¢œè‰²æ¯”å…¶ä»–éƒ¨ä½é¢œè‰²æ›´æ·±
                    å…·ä½“åŸå› ï¼šåå·ç§¯æ“ä½œæ—¶ï¼Œå·ç§¯æ ¸çš„å¤§å°ä¸èƒ½è¢«æ­¥é•¿æ•´é™¤ï¼Œåå·ç§¯è¾“å‡ºçš„ç»“æœå°±ä¼šä¸å‡åŒ€é‡å 
                åŸåˆ™ä¸Šï¼Œç½‘ç»œå¯ä»¥é€šè¿‡è®­ç»ƒè°ƒæ•´æƒé‡æ¥é¿å…æ­¤æƒ…å†µï¼Œè°ƒæ•´å¥½å·ç§¯æ ¸å¤§å°ä¸æ­¥é•¿ä¹‹é—´çš„å…³ç³»(ä¸é‡å ä¸å‡åŒ€é‡å å‡å¯é¿å…)
                    è¿˜å¯ä»¥è¿›è¡Œæ’å€¼Resizeï¼Œå†è¿›è¡Œåå·ç§¯æ“ä½œæ¥é¿å…
        (è½¬ç½®å·ç§¯çš„æ¦‚å¿µã€å®šä¹‰ã€å®ç°ç­‰ä»CSDNä¸­è§‚çœ‹æ›´åŠ ç›´è§‚)
    """

    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=2, s=2, p=0, bn=True, act=True):
        """Initialize ConvTranspose2d layer with batch normalization and activation function."""
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(c1, c2, k, s, p, bias=not bn)
        # bias=not bn: å¦‚æœbnä¸ºFalse(ä¸ä½¿ç”¨æ‰¹å½’ä¸€åŒ–),åˆ™æ·»åŠ åç½®é¡¹
        # è¿™æ˜¯å› ä¸ºæ‰¹å½’ä¸€åŒ–æœ¬èº«ä¼šæ·»åŠ ä¸€ä¸ªåç½®é¡¹ï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ä¸éœ€è¦é¢å¤–çš„åç½®é¡¹
        self.bn = nn.BatchNorm2d(c2) if bn else nn.Identity()
        # å¦‚æœbnå‚æ•°æ˜¯Trueï¼Œé‚£ä¹ˆåˆ›å»ºæ‰¹å½’ä¸€åŒ–å±‚ï¼›å¦‚æœç”¨æˆ·æŒ‡å®šbnä¸ºFalseï¼Œé‚£ä¹ˆä½¿ç”¨ç©ºå ä½ç¬¦nn.Identity()ï¼Œåˆ›å»ºf(x)=x
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

    def forward(self, x):   # è®­ç»ƒä¸æ¨ç†
        """Applies transposed convolutions, batch normalization and activation to input."""
        return self.act(self.bn(self.conv_transpose(x)))

    def forward_fuse(self, x):  # æ¨ç†
        """Applies activation and convolution transpose operation to input."""
        return self.act(self.conv_transpose(x))


class Focus(nn.Module): 
    """Focus wh information into c-space.
        ä¸»è¦åŠŸèƒ½æ˜¯é™ä½è¾“å…¥å¼ é‡çš„ç©ºé—´å°ºå¯¸(é«˜åº¦å’Œå®½åº¦)ï¼ŒåŒæ—¶å¢åŠ é€šé“æ•°ã€‚
        è¾“å…¥æ˜¯(B,C,H,W),è¾“å‡ºæ˜¯(B,4C,H/2,W/2)
        é™ä½ç©ºé—´åˆ†è¾¨ç‡ï¼ŒåŠ å¿«è®¡ç®—é€Ÿåº¦å¹¶é™ä½å†…å­˜ä½¿ç”¨é‡ï¼Œå°¤å…¶åœ¨å¤„ç†å¤§å‹å›¾åƒæ—¶
        é€šè¿‡è¿æ¥æ¯ä¸ªé€šé“ä¸­ä¸åŒç©ºé—´ä½ç½®çš„å…ƒç´ ï¼Œæ¨¡å‹å¯ä»¥æ½œåœ¨æ•æ‰åˆ°å›¾åƒä¸­ç‰¹å¾çš„æ›´å…¨é¢è¡¨ç¤º
    """

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):
        """Initializes Focus object with user defined channel, convolution, padding, group and activation values."""
        super().__init__()
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act=act)
        # self.contract = Contract(gain=2)

    def forward(self, x):
        """
        Applies convolution to concatenated tensor and returns the output.

        Input shape is (b,c,w,h) and output shape is (b,4c,w/2,h/2).
        å˜é‡ä¸­xçš„æœ¬æ¥å½¢çŠ¶æ˜¯(B,C,H,W)ï¼Œé€å…¥å·ç§¯æ—¶ç‰¹å¾å›¾çš„å½¢çŠ¶ä¸º(B,4C,H/2,W/2)
        å‰å‘ä¼ æ’­æ—¶å¯¹æ¯ä¸€ä¸ªé€šé“åˆ‡ç‰‡(,1,H,W)éƒ½è¿›è¡Œå››æ¬¡é‡‡æ ·ï¼Œå¹¶åœ¨é€šé“ç»´åº¦ä¸Šå åŠ ï¼Œå¾—åˆ°(,4,H/2,W/2)çš„åˆ‡ç‰‡ï¼Œå†æŠŠæ•´ä½“(B,4C,h/2,W/2)é€å…¥å·ç§¯
        """
        return self.conv(torch.cat((x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]), 1))
        # return self.conv(self.contract(x))
        # x[..., ::2, ::2]æ˜¯æŒ‡å–äº†å‰ä¸¤ä¸ªç»´åº¦Batchsizeå’ŒChannelçš„æ‰€æœ‰å€¼ï¼ŒæŒ‰ç…§ä¸€å®šæ­¥é•¿å–äº†Heightå’ŒWidthçš„éƒ¨åˆ†å€¼
        # x[..., ::2, ::2]å–å‡ºå¶æ•°è¡Œå’Œå¶æ•°åˆ—çš„å…ƒç´ 
        # x[..., 1::2, ::2]å–å‡ºå¥‡æ•°è¡Œå’Œå¶æ•°åˆ—çš„å…ƒç´ 
        # x[..., ::2, 1::2]å–å‡ºå¶æ•°è¡Œå’Œå¥‡æ•°åˆ—çš„å…ƒç´ 
        # x[..., 1::2, 1::2]å–å‡ºå¥‡æ•°è¡Œå’Œå¥‡æ•°åˆ—çš„å…ƒç´ 
        # torch.catå°†è¿™å››ä¸ªå­å¼ é‡æ²¿é€šé“ç»´åº¦(è½´1)è¿›è¡Œæ‹¼æ¥ã€‚æœ‰æ•ˆåœ°å°†æ¥è‡ªæ‰€æœ‰å››ç§é‡‡æ ·æ–¹å¼çš„ä¿¡æ¯ç»„åˆæˆä¸€ä¸ªå•ä¸€å¼ é‡ï¼Œè¯¥å¼ é‡é€šé“æ•°æ˜¯åŸå§‹é€šé“æ•°å››å€


class GhostConv(nn.Module):
    """Ghost Convolution https://github.com/huawei-noah/ghostnet.
        GhostConvå¹»å½±å·ç§¯ï¼Œæ—¨åœ¨é€šè¿‡å»‰ä»·æ“ä½œç”Ÿæˆæ›´å¤šçš„ç‰¹å¾å›¾ã€‚
        ç¥ç»ç½‘ç»œçš„ä¸´è¿‘å±‚ç»å¸¸ä¼šç”Ÿæˆä¸€äº›ç±»ä¼¼çš„ç‰¹å¾å›¾ï¼Œç”¨æ™®é€šå·ç§¯ç”Ÿæˆè¿™äº›ç±»ä¼¼çš„ç‰¹å¾å›¾å¾ˆè€—è´¹èµ„æº
        æ­¥éª¤ï¼šå…ˆè¿›è¡Œ1*1å·ç§¯èšåˆé€šé“é—´çš„ä¿¡æ¯ç‰¹å¾ï¼Œç„¶åå†ä½¿ç”¨åˆ†ç»„å·ç§¯ï¼Œç”Ÿæˆæ–°çš„ç‰¹å¾å›¾
        ä¸ºäº†å‡å°‘ç½‘ç»œè®¡ç®—é‡ï¼Œä½œè€…å°†ä¼ ç»Ÿå·ç§¯åˆ†ä¸ºä¸¤æ­¥åŠé€†è¡Œï¼Œé¦–å…ˆé€šè¿‡ä¼ ç»Ÿå·ç§¯ç”Ÿæˆchannelè¾ƒå°çš„ç‰¹å¾å›¾ä»¥å‡å°‘è®¡ç®—é‡ï¼Œ
            ç„¶ååœ¨å¾—åˆ°çš„ç‰¹å¾å›¾çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡cheap operation(depthwise convï¼Œå»‰ä»·æ“ä½œ)å†è¿›ä¸€æ­¥å‡å°‘è®¡ç®—é‡ï¼Œç”Ÿæˆæ–°çš„ç‰¹å¾å›¾
            æœ€åå°†ä¸¤ç»„ç‰¹å¾å›¾æ‹¼æ¥åˆ°ä¸€èµ·ï¼Œç”Ÿæˆæœ€ç»ˆçš„è¾“å‡º
        å·ç§¯æ“ä½œæ˜¯å·ç§¯-æ‰¹å½’ä¸€åŒ–BN-éçº¿æ€§æ¿€æ´»å…¨å¥—ç»„åˆï¼Œè€Œæ‰€è°“çš„çº¿æ€§å˜æ¢æˆ–å»‰ä»·æ“ä½œ(cheap operation)å‡æŒ‡æ™®é€šå·ç§¯ï¼Œä¸å«æ‰¹å½’ä¸€åŒ–å’Œéçº¿æ€§æ¿€æ´»
    """

    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):
        """Initializes the GhostConv object with input channels, output channels, kernel size, stride, groups and
        activation.
        """
        super().__init__()
        c_ = c2 // 2  # hidden channelsï¼Œè¾“å‡ºçš„ä¸€åŠ
        self.cv1 = Conv(c1, c_, k, s, None, g, act=act) # æ™®é€šå·ç§¯
        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act=act)    # æ·±åº¦å·ç§¯ï¼Œc_ä¸ºæ·±åº¦å·ç§¯çš„åˆ†ç»„
        # cv2ä½¿ç”¨çš„å·ç§¯æ ¸5*5çš„æ·±åº¦å·ç§¯å³æ˜¯ä½œè€…æ‰€è¨€çš„"å»‰ä»·çš„æ“ä½œ"

    def forward(self, x):
        """Forward propagation through a Ghost Bottleneck layer with skip connection."""
        y = self.cv1(x)
        return torch.cat((y, self.cv2(y)), 1)   # torch.cat()ï¼šåœ¨ç»™å®šç»´åº¦ä¸Šå¯¹è¾“å…¥çš„å¼ é‡åºåˆ—seqè¿›è¡Œè¿æ¥æ“ä½œ
        # torch.cat((y,self.cv2(y)),1)æ„å‘³ç€æŠŠyå’Œself.cv2(y)æ²¿ç€é€šé“çš„æ–¹å‘(1çš„æ–¹å‘)è¿æ¥
        # ç›¸å½“äºæ˜¯æŠŠcv1(x)å’Œcv2(cv1(x))æ²¿ç€é€šé“æ–¹å‘è¿æ¥
        # cv1(x)çš„é€šé“æ•°æ˜¯c_ï¼Œcv2(cv1(x))çš„é€šé“æ•°è¿˜æ˜¯c_ï¼Œç”±äºc_æ˜¯c2çš„ä¸€åŠï¼Œä¸¤ä¸ªé€šé“ç›¸åŠ ä¹‹åå°±å˜æˆäº†c2ï¼Œå³è¾“å‡ºé€šé“æ•°


class RepConv(nn.Module):
    """
    RepConv is a basic rep-style block, including training and deploy status.

    This module is used in RT-DETR.
    Based on https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py
    RepConvï¼šé‡å‚æ•°åŒ–å·ç§¯ã€‚å°†ä¼ ç»Ÿçš„å·ç§¯å±‚æ‹†åˆ†ä¸ºæ·±åº¦å·ç§¯å’Œé€ç‚¹å·ç§¯ï¼Œå³æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å‚æ•°qæ¥æ§åˆ¶ä¸¤è€…ä¹‹é—´çš„æ¯”ä¾‹ã€‚
        é€šè¿‡å®éªŒå‘ç°ï¼Œq=0.5æ—¶ï¼Œæ¨¡å‹å¯ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚
        è¿™ç§ç»“æ„é‡å‚æ•°åŒ–æ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡å°‘æ¨¡å‹çš„å‚æ•°é‡å’Œè®¡ç®—é‡ï¼ŒåŒæ—¶åˆä¸æŸå¤±ç²¾åº¦ã€‚
        RepConvåœ¨è®­ç»ƒæ—¶è¿˜ä½¿ç”¨å¤šåˆ†æ”¯ç»“æ„ï¼Œåœ¨æ¨ç†æ—¶å°†å¤šåˆ†æ”¯çš„å‚æ•°èåˆï¼Œä½¿ç”¨å•åˆ†æ”¯ç»“æ„ã€‚
            è¿™ç§è®­ç»ƒå¤šåˆ†æ”¯ã€æ¨ç†å•åˆ†æ”¯çš„ç»“æ„å¯ä»¥èŠ‚çœè®¡ç®—èµ„æºã€‚
    
    RepConvç±»åœ¨è®­ç»ƒçš„æ—¶å€™ä½¿ç”¨ä¸‰ä¸ªå¹¶åˆ—çš„åˆ†æ”¯(self.conv1æ˜¯3x3å·ç§¯ã€self.conv2æ˜¯1x1å·ç§¯ã€self.bnæ˜¯BNå±‚)ï¼Œ
    å‰å‘ä¼ æ’­ä½¿ç”¨forwardå‡½æ•°ï¼Œåœ¨é€šè¿‡åå‘ä¼ æ’­è®­ç»ƒå¥½ä¸‰ä¸ªåˆ†æ”¯çš„å‚æ•°åï¼Œ
    ä½¿ç”¨_fuse_bn_tensorå‡½æ•°ã€_pad_1x1_to_3x3_tensorå‡½æ•°ã€get_equivalent_kernel_biaså‡½æ•°ã€
    fuse_convså‡½æ•°å»èåˆä¸‰ä¸ªåˆ†æ”¯ï¼Œå¹¶è·å¾—èåˆåçš„å•åˆ†æ”¯(ä¸€å±‚å·ç§¯)ï¼Œç”¨äºæœ€åçš„æ¨ç†è¿‡ç¨‹
    """

    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=3, s=1, p=1, g=1, d=1, act=True, bn=False, deploy=False):  # deployæ˜¯å¦å¤„äºéƒ¨ç½²çŠ¶æ€(é»˜è®¤ä¸ºFalse)
        """Initializes Light Convolution layer with inputs, outputs & optional activation function."""
        super().__init__()
        assert k == 3 and p == 1
        # assertæ–­è¨€å…³é”®å­—ï¼Œå¦‚æœåé¢çš„æ¡ä»¶ä¸æ»¡è¶³(ä¸ä¸ºçœŸ)ï¼Œé‚£ä¹ˆä¼šè§¦å‘AssertionErrorå¼‚å¸¸
        # å³k=3,p=1è¿™ä¸¤ä»¶äº‹å¿…é¡»åŒæ—¶å‘ç”Ÿï¼Œå¦åˆ™ä¼šå¼•å‘å¼‚å¸¸
        self.g = g
        self.c1 = c1
        self.c2 = c2
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

        self.bn = nn.BatchNorm2d(num_features=c1) if bn and c2 == c1 and s == 1 else None
        # å¦‚æœç”¨æˆ·ç»™çš„å‚æ•°bnä¸ä¸ºFalseè€Œä¸”è¾“å…¥é€šé“æ•°c1ç­‰äºè¾“å‡ºé€šé“æ•°c2è€Œä¸”å·ç§¯æ­¥å¹…s=1
        # é‚£ä¹ˆself.bnæ˜¯ä¸€å±‚ç”¨nn.BatchNorm2då®ç°çš„BNå±‚ï¼Œé€šé“æ•°ä¸ºc1
        # ä¸Šè¿°é™å®šæ¡ä»¶ä¹‹ä¸€æœ‰ä¸€ä¸ªä¸æ»¡è¶³ï¼Œåˆ™self.bnæ˜¯ç©º(None)
        self.conv1 = Conv(c1, c2, k, s, p=p, g=g, act=False)    # æ™®é€šå·ç§¯ï¼Œä¸”ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
        self.conv2 = Conv(c1, c2, 1, s, p=(p - k // 2), g=g, act=False) # 1*1å·ç§¯(é€ç‚¹å·ç§¯)
        # k=3ä¸”p=1ï¼Œå› æ­¤p=1-3//2=0ï¼Œå³pä¸º0
        # 1*1å·ç§¯ä¸ä¼šæ”¹å˜ç‰¹å¾å›¾çš„Hå€¼å’ŒWå€¼ï¼Œå¡«å……å€¼å¯ä»¥ç›´æ¥ç»™0ï¼Œä½†æ˜¯ä¸ºäº†é˜²èŒƒä¸€äº›è¾¹ç•Œæ•ˆåº”ï¼Œä½¿ç”¨p=(p-k//2)ä½œä¸º1*1å·ç§¯çš„å¡«å……
        # è¾¹ç•Œæ•ˆåº”å³ç”±äºå·ç§¯æ“ä½œä¼šåœ¨è¾“å…¥ç‰¹å¾å›¾çš„è¾¹ç•Œå¤„å¼•å…¥ä¸€äº›é¢å¤–çš„å€¼ï¼Œä»è€Œå¯¼è‡´è¾“å‡ºç‰¹å¾å›¾çš„è¾¹ç¼˜åƒç´ ä¸å†…éƒ¨åƒç´ å­˜åœ¨å·®å¼‚

    def forward_fuse(self, x):  # (æ— BNä¸”ä½†åˆ†æ”¯)ï¼Œæ¨ç†æ—¶ä½¿ç”¨ï¼Œå·²ç»èåˆåçš„å‚æ•°
        """Forward process."""
        return self.act(self.conv(x))

    def forward(self, x):   # (æœ‰BNä¸”ä¸‰åˆ†æ”¯)ï¼Œè®­ç»ƒæ—¶ä½¿ç”¨ï¼Œæœªèåˆå‚æ•°
        """Forward process."""
        id_out = 0 if self.bn is None else self.bn(x)
        # å½“self.bn(x)ä¸ä¸ºç©ºæ—¶ï¼Œid_outä¸ºself.bn(x)ï¼Œç›¸å½“äºç»è¿‡äº†BNå±‚çš„æ®‹å·®è¿æ¥
        # å¦‚æœself.bnä¸ºç©ºï¼Œid_outä¸º0
        return self.act(self.conv1(x) + self.conv2(x) + id_out)

    def get_equivalent_kernel_bias(self):
        """Returns equivalent kernel and bias by adding 3x3 kernel, 1x1 kernel and identity kernel with their biases.
            å°†ä¸åŒå·ç§¯åˆ†æ”¯çš„å‚æ•°èåˆï¼ŒæŠŠä¸‰ä¸ªåˆ†æ”¯å˜æˆä¸€ä¸ªç®€å•çš„å·ç§¯åˆ†æ”¯ï¼Œå‡å°‘å‚æ•°ï¼ŒèŠ‚çœè®¡ç®—èµ„æº
        """
        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1)   # 3*3å·ç§¯åˆ†æ”¯å†…éƒ¨çš„å·ç§¯å±‚å‚æ•°å’ŒBNå±‚å‚æ•°èåˆ
        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2)   # 1*1å·ç§¯åˆ†æ”¯å†…éƒ¨çš„å·ç§¯å±‚å‚æ•°å’ŒBNå±‚å‚æ•°èåˆ
        kernelid, biasid = self._fuse_bn_tensor(self.bn)    # BNå±‚å†…éƒ¨çš„å·ç§¯å±‚å‚æ•°å’ŒBNå±‚èåˆ
        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid
        # è¿”å›ä¸€ä¸ªæ–°æƒé‡gamma_newå’Œæ–°åå·®beta_newï¼ŒäºŒè€…éƒ½æ˜¯å°†ä¸‰ä¸ªåˆ†æ”¯çš„æƒé‡/åå·®ç›¸åŠ å¾—åˆ°

    def _pad_1x1_to_3x3_tensor(self, kernel1x1):
        """Pads a 1x1 tensor to a 3x3 tensor.
            å°†å·ç§¯æ ¸ä»1*1æ‰©å¼ æˆ3*3
            ä¸ºäº†æ–¹ä¾¿èåˆæ“ä½œï¼Œå°†ä¸åŒå¤§å°çš„å·ç§¯æ ¸ç»Ÿä¸€ä¸ºç›¸åŒçš„å¤§å°
        """
        if kernel1x1 is None:
            return 0
        else:
            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])
            # åœ¨kernel1x1çš„ä¸Šä¸‹å·¦å³å››ä¸ªæ–¹å‘å‘ä¸Šå¡«å……1ä¸ªåƒç´ ï¼Œæˆäº†3*3

    def _fuse_bn_tensor(self, branch):
        """Generates appropriate kernels and biases for convolution by fusing branches of the neural network.
            å°†BNå±‚å‚æ•°èåˆè¿›Convå±‚å‚æ•°
            _fuse_bn_tensorå‡½æ•°æ¥æ”¶ä¸€ä¸ªå‚æ•°branchï¼Œè¡¨ç¤ºéœ€è¦èåˆçš„ç½‘ç»œåˆ†æ”¯ï¼Œè¯¥branchå¯ä»¥æ˜¯self.conv1è¿™æ ·çš„Convç±»å®ä¾‹
                è¯¥å‡½æ•°çš„åŠŸèƒ½æ˜¯æ ¹æ®è¾“å…¥çš„branchä¿¡æ¯ï¼Œç”Ÿæˆåˆé€‚çš„å·ç§¯æ ¸å’Œåç½®
        """
        if branch is None:  # æ²¡æœ‰åˆ†æ”¯éœ€è¦èåˆï¼Œç›´æ¥è¿”å›0å’Œ0
            return 0, 0
        if isinstance(branch, Conv):    # å¦‚æœbranchå±‚æ˜¯Convç±»(åŒ…æ‹¬å·ç§¯å±‚ã€BNå±‚ã€æ¿€æ´»å±‚)
        # ä»branchä¸­å–å‡ºConvå±‚å’ŒBatchNormå±‚å¯¹åº”çš„å±æ€§å€¼ï¼Œå¹¶ä¿å­˜åœ¨æœ¬å‡½æ•°(_fuse_bn_tensor)çš„å˜é‡ä¸­
            kernel = branch.conv.weight # å·ç§¯å±‚çš„æƒé‡
            # kernelæ˜¯ä¸€ä¸ªå¤§å°ä¸º(out_channels, in_channels//groups, kernel_size[0], kernel_size[1])çš„å¼ é‡
            # å°†æ‰€æœ‰ç»„çš„æ»¤æ³¢å™¨æƒé‡åœ¨out_channelsæ–¹å‘å †å åœ¨ä¸€èµ·ï¼Œæœ€åçš„æƒé‡å³ä¸ºå¦‚ä¸Šæ‰€ç¤º
            running_mean = branch.bn.running_mean   # BatchNormå±‚çš„è¿è¡Œå¹³å‡å€¼
            running_var = branch.bn.running_var # BatchNormå±‚çš„è¿è¡Œæ–¹å·®
            gamma = branch.bn.weight    # BatchNormå±‚çš„æƒé‡
            beta = branch.bn.bias   # BatchNormå±‚çš„åç½®
            eps = branch.bn.eps # BatchNormå±‚çš„å¾®å°å€¼ï¼Œé˜²æ­¢æ–¹å·®åˆ†æ¯ä¸º0
        elif isinstance(branch, nn.BatchNorm2d):    # å¦‚æœbranchæ˜¯nn.BatchNorm2dç±»(åªæœ‰BNå±‚)
            if not hasattr(self, "id_tensor"):  # åˆ›å»ºå•ä½çŸ©é˜µ
                # å•ä½çŸ©é˜µçš„ä½œç”¨æ˜¯è¡¨ç¤ºä¸€ä¸ªæ’ç­‰æ˜ å°„(identity mapping)ï¼Œå³è¾“å…¥æ•°æ®ç›´æ¥ä¼ é€’åˆ°è¾“å‡ºï¼Œç›¸å½“äºå®ç°äº†æ®‹å·®è¿æ¥
                # æ£€æŸ¥ç±»æœ¬èº«æ˜¯å¦æ‹¥æœ‰å±æ€§id_tensor(å•ä½çŸ©é˜µ)ï¼Œå¦‚æœæ²¡æœ‰çš„è¯æ‰§è¡Œä¸‹é¢çš„ä»£ç å—(ç»§ç»­åˆ›å»ºä¸€ä¸ªå•ä½çŸ©é˜µid_tensor)
                input_dim = self.c1 // self.g   # æ ¹æ®è¾“å…¥é€šé“ä¸åˆ†ç»„æ•°è®¡ç®—å•ä½çŸ©é˜µå¤§å°
                kernel_value = np.zeros((self.c1, input_dim, 3, 3), dtype=np.float32)
                # åˆ›å»ºä¸Convç±»çš„æƒé‡ç›¸åŒå¤§å°çš„æƒé‡
                for i in range(self.c1):    # é€šè¿‡å¾ªç¯éå†å°†æŒ‡å®šä½ç½®çš„å…ƒç´ è®¾ç½®ä¸º1
                    kernel_value[i, i % input_dim, 1, 1] = 1
                    # i%input_dimæŒ‡iå¯¹input_dimå–ä½™ï¼Œåœ¨éå†input_dimçš„åŒæ—¶ï¼Œè®©è¯¥ç»´åº¦ä¸è¶…è¿‡input_dim
                    # å³å°†(out_channels, in_channels//groups, 1, 1)æ¯ä¸€ä¸ªæ»¤æ³¢å™¨çš„ä¸­å¿ƒä½ç½®è®¾ç½®ä¸º1ï¼Œå…¶ä»–å€¼è®¾ç½®ä¸º0
                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)
                # å°†numpyæ•°ç»„è½¬æ¢ä¸ºäº†pytorchå¼ é‡ï¼Œå¹¶æŠŠpytorchå¼ é‡ç§»åŠ¨åˆ°ä¸åˆ†æ”¯æƒé‡å¼ é‡ç›¸åŒçš„è®¾å¤‡ä¸Š
                # ç¡®ä¿å¼ é‡ä½äºåŒä¸€è®¾å¤‡ä¸Šï¼Œä»¥ä¾¿é«˜æ•ˆè®¡ç®—
            kernel = self.id_tensor
            running_mean = branch.running_mean
            running_var = branch.running_var
            gamma = branch.weight
            beta = branch.bias
            eps = branch.eps
        # æ ¡æ­£æƒé‡ä¸åå·®ï¼Œå°†BNå±‚çš„å‚æ•°èåˆè¿›Convå±‚
        # BNå±‚çš„å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥è®¡ç®—å‡ºæ¥ï¼Œåœ¨æ­¤å¤„å°†è®¡ç®—å‡ºçš„BNå‚æ•°ä¸å·ç§¯å‚æ•°èåˆï¼Œå¾—åˆ°æ–°çš„å·ç§¯å‚æ•°
        # è€Œæ¨ç†æ—¶ä¸éœ€è¦é‡æ–°è®¡ç®—ä¸€éBNçš„å‚æ•°ï¼Œç›´æ¥ä½¿ç”¨èåˆåçš„å‚æ•°å°±å¯ä»¥å®ç°BNçš„åŠŸèƒ½ï¼Œæ–°æƒé‡å’Œæ–°åå·®å°†ç”¨äºæ¨ç†æ—¶çš„å·ç§¯æ“ä½œã€‚
        std = (running_var + eps).sqrt()
        t = (gamma / std).reshape(-1, 1, 1, 1)  # å°†tå˜é‡é‡å¡‘ä¸ºäº†(-1,1,1,1)çš„å¤§å°ï¼Œå³å¹¿æ’­å¼ é‡
        # å¹¿æ’­å¼ é‡å…è®¸å½¢çŠ¶ä¸åŒçš„å¼ é‡è¿›è¡Œå…ƒç´ çº§ä¹˜æ³•ï¼Œé‡å¡‘åçš„tå¯ä»¥ä»»æ„ä¸kernelç›¸ä¹˜ã€‚å¦‚æœä¸é‡å¡‘ï¼ŒäºŒè€…çš„æƒ©ç½šå°†å¯¼è‡´ç»´åº¦ä¸åŒ¹é…å’Œé”™è¯¯
        return kernel * t, beta - running_mean * gamma / std    # å‡½æ•°è¿”å›æƒé‡æ ¡æ­£å€¼gamma'å’Œåå·®æ ¡æ­£å€¼beta'
        # åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—ä¸Šè¿°çš„å†…å®¹ï¼Œè¿™æ˜¯èåˆå‰çš„ï¼›èåˆågamma'å’Œbeta'æˆäº†å·²çŸ¥çš„ï¼Œåœ¨æ¨ç†æ—¶ç›´æ¥æ‹¿æ¥ç®—ï¼Œçœå»å¾ˆå¤šè®¡ç®—æ­¥éª¤

    def fuse_convs(self):
        """Combines two convolution layers into a single layer and removes unused attributes from the class.
            åˆå¹¶ä¸¤ä¸ªå·ç§¯å±‚(self.conv1å’Œself.conv2)æˆå•ä¸ªå·ç§¯å±‚(self.conv)ï¼Œå¹¶ç§»é™¤åˆå¹¶è¿‡ç¨‹ä¸­ä¸å†ä½¿ç”¨çš„å±æ€§
            åˆ é™¤æœªä½¿ç”¨å±æ€§ï¼Œå¯é‡Šæ”¾é¢å¤–çš„å†…å­˜ç©ºé—´ï¼Œè¿›ä¸€æ­¥é™ä½ç½‘ç»œçš„å†…å­˜å ç”¨ï¼Œè¿™å¯¹äºå¤§å‹æˆ–å¤æ‚ç½‘ç»œå°¤ä¸ºé‡è¦ï¼›
            è¿˜å¯ä»¥ç®€åŒ–ä»£ç ç»“æ„ï¼Œä½¿å…¶æ›´å®¹æ˜“ç†è§£å’Œç»´æŠ¤ã€‚
        """
        if hasattr(self, "conv"):   # å¦‚æœæœ‰convå±æ€§ï¼Œåˆå¹¶è¿‡ç¨‹å¯èƒ½å·²ç»å®Œæˆï¼Œå‡½æ•°ç›´æ¥è¿”å›
            return
        kernel, bias = self.get_equivalent_kernel_bias()    # èåˆåˆ†æ”¯ï¼Œå¾—åˆ°ç­‰ä»·çš„æƒé‡å’Œåå·®
        self.conv = nn.Conv2d(  # åˆ›å»ºæ–°çš„å·ç§¯å±‚å¹¶ä¼ å‚ï¼Œä½¿ç”¨çš„éƒ½æ˜¯conv1çš„å‚æ•°ï¼Œconv1ä¸­convå–å‡ºçš„å±æ€§éƒ½æ˜¯ç”¨æˆ·åœ¨åˆ›å»ºRepConvä¸­åˆå§‹åŒ–çš„
            in_channels=self.conv1.conv.in_channels,
            out_channels=self.conv1.conv.out_channels,
            kernel_size=self.conv1.conv.kernel_size,
            stride=self.conv1.conv.stride,
            padding=self.conv1.conv.padding,
            dilation=self.conv1.conv.dilation,
            groups=self.conv1.conv.groups,
            bias=True,
        ).requires_grad_(False) # requires_grad_å±æ€§è®¾ç½®ä¸ºFalseï¼Œè¡¨ç¤ºå…¶æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šæ›´æ–°
        self.conv.weight.data = kernel  # å°†èåˆåˆ†æ”¯çš„æƒé‡å’Œåå·®åˆ†åˆ«èµ‹å€¼ç»™æ–°åˆ›å»ºçš„å·ç§¯å±‚çš„æƒé‡å’Œåå·®
        self.conv.bias.data = bias
        for para in self.parameters():  # åˆ†ç¦»è®¡ç®—å›¾ï¼Œå¹¶åˆ é™¤æ— ç”¨çš„å±æ€§
            para.detach_()  # detach_æ–¹æ³•åˆ†ç¦»å±æ€§ä¸è®¡ç®—å›¾çš„è¿æ¥ï¼Œå‚æ•°çš„å†…å­˜åˆ†é…ä¸å†ç”±å›¾ç®¡ç†ï¼Œå¯ä»¥å®‰å…¨é‡Šæ”¾
            # å¦‚æœå‚æ•°åœ¨ç½‘ç»œä¸­ä¸å†ä½¿ç”¨ï¼Œå°±ä¼šæˆä¸ºä¸å¿…è¦çš„è´Ÿæ‹…ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜æ³„æ¼ï¼Œå³ç½‘ç»œä¿ç•™äº†æœªä½¿ç”¨çš„å†…å­˜èµ„æºï¼Œå¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½é—®é¢˜å’Œä¸ç¨³å®š
        self.__delattr__("conv1")   # __delattr__æ–¹æ³•åˆ é™¤ä¸å†éœ€è¦çš„å±æ€§ï¼ŒåŒ…æ‹¬conv1ã€conv2ã€nmã€bnã€id_tensor
        self.__delattr__("conv2")
        if hasattr(self, "nm"):
            self.__delattr__("nm")
        if hasattr(self, "bn"):
            self.__delattr__("bn")
        if hasattr(self, "id_tensor"):
            self.__delattr__("id_tensor")


class ChannelAttention(nn.Module):
    """Channel-attention module https://github.com/open-mmlab/mmdetection/tree/v3.0.0rc1/configs/rtmdet."""

    def __init__(self, channels: int) -> None:  # channelsè¾“å…¥å¼ é‡çš„é€šé“æ•°
        """Initializes the class and sets the basic configurations and instance variables required."""
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1) # è‡ªé€‚åº”å¹³å‡æ± åŒ–å±‚ï¼Œå°†è¾“å…¥ç‰¹å¾å›¾ç¼©å°åˆ°ç©ºé—´å°ºå¯¸ä¸º1*1
        self.fc = nn.Conv2d(channels, channels, 1, 1, 0, bias=True) # ç”¨å·ç§¯å±‚å®ç°å…¨è¿æ¥å±‚çš„åŠŸèƒ½
        # ç”¨äºå°†æ± åŒ–åçš„ç‰¹å¾å›¾ä¸­çš„ä¿¡æ¯è½¬æ¢ä¸ºé€šé“æ³¨æ„åŠ›æƒé‡ã€‚(è¾“å…¥è¾“å‡ºé€šé“æ•°ç›¸åŒï¼Œå†…æ ¸å¤§å°1*1ï¼Œæ— å¡«å……ï¼Œå¸¦æœ‰åç½®é¡¹)
        self.act = nn.Sigmoid() # å°†æ³¨æ„åŠ›æƒé‡å‹ç¼©è‡³0-1

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies forward pass using activation on convolutions of the input, optionally using batch normalization."""
        return x * self.act(self.fc(self.pool(x)))
        # self.act(self.fc(self.pool(x)))å¾—åˆ°æ³¨æ„åŠ›æƒé‡ã€‚
        # æ³¨æ„åŠ›æƒé‡ä»¥é€å…ƒç´ ä¹˜æ³•çš„æ–¹å¼ä¸åŸå§‹ç‰¹å¾å›¾xç›¸ä¹˜ï¼Œç›¸å½“äºæ”¾å¤§æ¨¡å‹å…³æ³¨çš„é‡è¦çš„é€šé“ä¿¡æ¯ï¼ŒåŒæ—¶å‰Šå¼±ä¸é‡è¦çš„é€šé“ä¿¡æ¯


class SpatialAttention(nn.Module):
    """Spatial-attention module.
        ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¸®åŠ©æ¨¡å‹åœ¨å¤„ç†å›¾åƒæ—¶æ›´åŠ å…³æ³¨é‡è¦çš„ç©ºé—´ä¿¡æ¯
    """

    def __init__(self, kernel_size=7):
        """Initialize Spatial-attention module with kernel size argument."""
        super().__init__()
        assert kernel_size in {3, 7}, "kernel size must be 3 or 7"  # æ–­è¨€ï¼Œè§„å®šå·ç§¯æ ¸å¤§å°ä¸€å®šä¸º3æˆ–7
        padding = 3 if kernel_size == 7 else 1  # k=3,p=1æˆ–è€…k=7,p=3ä¿è¯å·ç§¯æ“ä½œåç‰¹å¾å›¾çš„Hå’ŒWä¸å˜
        self.cv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)    # è¾“å…¥é€šé“2è¾“å‡ºé€šé“1çš„æ™®é€šå·ç§¯
        self.act = nn.Sigmoid()

    def forward(self, x):
        """Apply channel and spatial attention on input for feature recalibration."""
        return x * self.act(self.cv1(torch.cat([torch.mean(x, 1, keepdim=True), torch.max(x, 1, keepdim=True)[0]], 1)))
        # è¾“å…¥å¼ é‡å¤§å°ä¸º(B,C,H,W)
        # torch.mean(x,1,keepdim=True)è¿”å›xåœ¨é€šé“ç»´åº¦çš„å¹³å‡å€¼ï¼Œè¾“å‡ºä¸º(B,1,H,W)
        # torch.max(x,1,keepdim=True)[0]è¿”å›xçš„é€šé“æœ€å¤§å€¼ï¼Œè¾“å‡ºä¸º(B,1,H,W).(æŠ›å¼ƒäº†[1]çš„é€šé“æœ€å¤§å€¼ç´¢å¼•)
        # torch.catæ“ä½œå°†å¹³å‡å€¼å’Œæœ€å¤§å€¼è¿æ¥ï¼Œå¾—åˆ°(B,2,H,W)
        # å†å°†(B,2,H,W)é€å…¥å·ç§¯å’Œæ¿€æ´»ï¼Œå¾—åˆ°(B,1,H,W)çš„ç©ºé—´æ³¨æ„åŠ›æƒé‡å€¼ã€‚å·ç§¯æ˜¯ä¸ºäº†æµ“ç¼©ç©ºé—´ä¿¡æ¯ï¼Œæ¿€æ´»æ˜¯ä¸ºäº†æ˜ å°„
        # æœ€åå°†æƒé‡ä¸xè¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ï¼Œæ”¾å¤§æ¨¡å‹å…³æ³¨çš„é‡è¦çš„ç©ºé—´ä¿¡æ¯ï¼Œå‰Šå¼±ä¸é‡è¦çš„ç©ºé—´ä¿¡æ¯


class CBAM(nn.Module):
    """Convolutional Block Attention Module.
        å…ˆé€šè¿‡é€šé“æ³¨æ„åŠ›ï¼Œå†é€šè¿‡ç©ºé—´æ³¨æ„åŠ›
    """

    def __init__(self, c1, kernel_size=7):
        """Initialize CBAM with given input channel (c1) and kernel size."""
        super().__init__()
        self.channel_attention = ChannelAttention(c1)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        """Applies the forward pass through C1 module."""
        return self.spatial_attention(self.channel_attention(x))


class Concat(nn.Module):
    """Concatenate a list of tensors along dimension.
        é€šé“è¿æ¥ï¼Œæ²¿æŒ‡å®šçš„ç»´åº¦
    """

    def __init__(self, dimension=1):
        """Concatenates a list of tensors along a specified dimension."""
        super().__init__()
        self.d = dimension  # æŒ‡å®šäº†è¿æ¥å¼ é‡çš„ç»´åº¦
        # self.d=1è¡¨ç¤ºæ²¿é€šé“ç»´åº¦è¿›è¡Œè¿æ¥
        # self.d=2è¡¨ç¤ºæ²¿é«˜åº¦ç»´åº¦è¿›è¡Œè¿æ¥

    def forward(self, x):   # xæ˜¯ä¸€ä¸ªåŒ…å«è¦è¿æ¥çš„å¼ é‡åˆ—è¡¨çš„è¾“å…¥
        """Forward pass for the YOLOv8 mask Proto module."""
        return torch.cat(x, self.d)
        # è¿”å›xæ²¿self.dæŒ‡å®šçš„æ–¹å‘è¿›è¡Œè¿æ¥
